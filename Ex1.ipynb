{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The (modified) Cliff Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Cliff environment is an ideal task to test risk-aware rl algorithms. It consists in a gridworld where the shortest path from the initial position to the goal position is close to a cliff. \n",
    "\n",
    "Assumptions:\n",
    "\n",
    "- We are on a slope: with probability 0.025 we will slip toward the cliff\n",
    "- There is a punishment of -50 for trying to get ouside the gridworld\n",
    "- Once the agent falls from the cliff he dies. While dying he collect a reward of -50. Perhaps dying is not a good experience, afterall\n",
    "- If the agent arrives to the shelter (goal), positioned on the other side of the cliff, he gets a reward of 500. Since the motivation of having a beer to the shelter can vary a lot from person to person, this reward can be changed by the user, to experience how it affects on the risk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class Cliff:\n",
    "    def __init__(self, x_size=20, y_size=10, prob=0.025, max_reward=500):\n",
    "        self.x_size = x_size\n",
    "        self.y_size = y_size\n",
    "        self.prob = prob\n",
    "        self.my_pos = np.array([0,0])           # bottom-left\n",
    "        self.act_meaning = [np.array([-1,0]),   # action 0 = left\n",
    "                            np.array([1,0]),    # action 1 = right\n",
    "                            np.array([0,1]),    # action 2 = up\n",
    "                            np.array([0,-1])    # action 3 = down\n",
    "                           ]\n",
    "        self.max_reward = max_reward\n",
    "        \n",
    "    def cartesian_to_int(self, x, y):\n",
    "        return y * self.x_size + x\n",
    "    \n",
    "    def int_to_cartesian(self, state):\n",
    "        return state // self.x_size, state % self.x_size\n",
    "    \n",
    "    def step(self, a):\n",
    "        action = np.copy(a)\n",
    "        prew_pos = np.copy(self.my_pos)\n",
    "        \n",
    "        if np.random.rand() < self.prob:\n",
    "            action = 3\n",
    "        \n",
    "        self.my_pos += self.act_meaning[action]\n",
    "        \n",
    "        reward = -1\n",
    "        terminate = False\n",
    "        if self.my_pos[1] < 0: # down the cliff\n",
    "            reward = -50\n",
    "            terminate = True\n",
    "            \n",
    "        if self.my_pos[1] >= self.y_size or self.my_pos[0] <= -1 or self.my_pos[0] >= self.x_size:\n",
    "            self.my_pos = prew_pos\n",
    "            reward = -50\n",
    "        \n",
    "        if self.my_pos[0] == self.x_size - 1 and self.my_pos[1] == 0: # Goal!\n",
    "            reward = self.max_reward\n",
    "            terminate = True\n",
    "            \n",
    "        return self.cartesian_to_int(*self.my_pos), reward, terminate\n",
    "    \n",
    "    def check_valid(self, x,y):\n",
    "        return y < self.y_size and y >=0 and x < self.x_size and x >=0\n",
    "    \n",
    "    def reset(self):\n",
    "        self.my_pos = np.array([0,0])\n",
    "        return self.cartesian_to_int(*self.my_pos)\n",
    "        \n",
    "class QLearning:\n",
    "    \n",
    "    def __init__(self, update_rule, n_states=200, n_actions=4, gamma=0.99, initialization=500):\n",
    "        \n",
    "        self.Q = np.ones((n_states, n_actions), dtype=np.float32) * initialization # optimistic update\n",
    "        self.N = np.ones((n_states, n_actions))\n",
    "        self.update_rule = update_rule\n",
    "        self.gamma = gamma\n",
    "        \n",
    "    def get_learning_rate(self, state, action):\n",
    "        return 1. / (self.N[state, action])\n",
    "    \n",
    "    def update(self, s, a, r, s_t, t):\n",
    "        alpha = self.get_learning_rate(s, a)\n",
    "        # we have to implement the update rule\n",
    "        self.Q[s,a] = self.update_rule(self.Q, s, a, r, s_t, t, alpha, self.gamma)\n",
    "        self.N[s,a] = self.N[s,a] + 1\n",
    "        \n",
    "    def act(self, state, exploit=False):\n",
    "        if exploit:\n",
    "            return int(np.asscalar(np.argmax(self.Q[state,:])))\n",
    "        else:\n",
    "            return int(np.asscalar(np.argmin(self.N[state,:])))\n",
    "\n",
    "def plot_cliff(cliff, Q,  n_visits, title=\"\"):\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tick_params(\n",
    "        axis='both',\n",
    "        which='both',      \n",
    "        bottom=False,      \n",
    "        top=False,\n",
    "        left=False,         \n",
    "        labelbottom=False,\n",
    "        labelleft=False) \n",
    "    \n",
    "    plt.axis([0, cliff.x_size, 0, cliff.y_size])\n",
    "    plt.xticks(range(cliff.x_size))\n",
    "    plt.yticks(range(cliff.y_size))\n",
    "    plt.text(0.5, -0.5, \"Start\", fontsize=12, \n",
    "             horizontalalignment='center',\n",
    "             verticalalignment='center')\n",
    "    plt.text(cliff.x_size - 0.5, -0.5, \"Goal\", fontsize=12, \n",
    "             horizontalalignment='center',\n",
    "             verticalalignment='center')\n",
    "    if n_visits is not None:\n",
    "        plt.imshow(np.flip(n_visits.T, axis=0), \n",
    "                cmap='Blues', \n",
    "                extent=(0, cliff.x_size, 0, cliff.y_size))\n",
    "\n",
    "    for col in range(cliff.x_size):\n",
    "        for row in range(cliff.y_size):\n",
    "            action = np.argmax(Q[cliff.cartesian_to_int(col,row), :])\n",
    "            x_act = cliff.act_meaning[action][0]\n",
    "            y_act = cliff.act_meaning[action][1]\n",
    "            plt.arrow(col + 0.5, row +0.5, 0.3 * x_act, 0.3 * y_act, width=0.04)\n",
    "                \n",
    "class Learning:\n",
    "    \n",
    "    def __init__(self, algorithm, mdp):\n",
    "        self.algorithm = algorithm\n",
    "        self.mdp = mdp\n",
    "        \n",
    "    def evaluate(self, n_steps=2000):\n",
    "        visits = np.zeros((self.mdp.x_size, self.mdp.y_size))\n",
    "        state = self.mdp.reset()\n",
    "        rewards = []\n",
    "        tot_reward = 0.\n",
    "        n=0\n",
    "        n_cliff = 0\n",
    "        step_per_episode = 0\n",
    "        while n < n_steps:\n",
    "            step_per_episode += 1\n",
    "            visits[self.mdp.my_pos[0],self.mdp.my_pos[1]] =\\\n",
    "                    visits[self.mdp.my_pos[0],self.mdp.my_pos[1]] + 1\n",
    "            action = self.algorithm.act(state, exploit=True)\n",
    "            state, reward, terminate = self.mdp.step(action)\n",
    "            tot_reward += reward\n",
    "            if terminate or step_per_episode >= 500: \n",
    "                step_per_episode = 0\n",
    "                if terminate and reward==-50:\n",
    "                    n_cliff += 1\n",
    "                if self.mdp.check_valid(*self.mdp.my_pos):\n",
    "                    visits[self.mdp.my_pos[0],self.mdp.my_pos[1]] =\\\n",
    "                        visits[self.mdp.my_pos[0],self.mdp.my_pos[1]] + 1\n",
    "                state = self.mdp.reset()\n",
    "                rewards.append(tot_reward)\n",
    "                tot_reward=0.\n",
    "            n+=1\n",
    "        return (rewards if len(rewards)>0 else [-2000]), visits, n_cliff\n",
    "    \n",
    "    def learn(self, n_steps=30000):\n",
    "        state = self.mdp.reset()\n",
    "        for _ in range(n_steps):\n",
    "            action = self.algorithm.act(state, exploit=False)\n",
    "            next_state, reward, terminate = self.mdp.step(action)\n",
    "            self.algorithm.update(state, action, reward, next_state, terminate)\n",
    "            state = next_state\n",
    "            if terminate: \n",
    "                state = self.mdp.reset()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The classical Q-Learning Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classical_update(Q, s, a, r, s_t, t, alpha, gamma):\n",
    "    q_next = np.asscalar(np.max(Q[s_t,:])) if not t else 0\n",
    "    return (1-alpha) * Q[s,a] + alpha * (r + gamma * q_next)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The first experiment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment(q_update_rule, max_reward=500):\n",
    "    q_learning = QLearning(q_update_rule, initialization=max_reward)\n",
    "    cliff = Cliff(max_reward=max_reward)\n",
    "    learner = Learning(q_learning, cliff)\n",
    "    Js = []\n",
    "    for _ in range(50):\n",
    "        print(\".\",end='')\n",
    "        Js.append(np.mean(learner.evaluate()[0]))\n",
    "        learner.learn()\n",
    "\n",
    "    plt.plot(Js)\n",
    "    plt.xlabel(\"n epochs\")\n",
    "    plt.ylim(-500, 1000)\n",
    "    h = plt.ylabel(\"E[J]\")\n",
    "    h.set_rotation(0)\n",
    "    plt.show()\n",
    "    \n",
    "    # evaluation of the last policy\n",
    "    last_eval = learner.evaluate(n_steps=10000)\n",
    "    plot_cliff(cliff, q_learning.Q, last_eval[1])\n",
    "    plt.show()\n",
    "    \n",
    "    plt.title(\"Distribution of the Return\")\n",
    "    plt.hist(last_eval[0],bins=100)\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Number of fallings from a cliff %d/%d\" % (last_eval[2],len(last_eval[0])))\n",
    "    \n",
    "experiment(classical_update)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Implement a risk update rule, and execute it! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beta_update(Q, s, a, r, s_t, t, alpha, gamma, beta=0.1):\n",
    "    def beta_update_core(Q, s, a, r, s_t, t, alpha, gamma):\n",
    "        #TODO: Implement!\n",
    "        return None\n",
    "    return beta_update_core\n",
    "\n",
    "def hat_update(Q, s, a, r, s_t, t, alpha, gamma):\n",
    "    #TODO: Implement!\n",
    "    return None\n",
    "\n",
    "experiment(hat_update, max_reward=1000)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
